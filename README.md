# Predicting a YouTube Video's Growth Rate

We aim to predict virality by using the percentage change in views between the second and sixth hour since a video's publishing. We use several video features such as thumbnail image, video title, channel parameters, duration, and more.

## I. Introduction

The success of content creators on YouTube is largely driven by the number of views their videos receive; views are a direct measure of engagement and help determine a video's earning potential. Consequently, content creators try to forecast the virality of their videos as it might indicate eventual success and overall channel health. In this project, we aim to predict virality by using the percentage change in views between the second and sixth hour since a video's publishing. We use several video features such as thumbnail image, video title, channel parameters, duration, and more.

## II. Methodology

### a. Preprocessing
#### i. Exploratory Data Analysis (EDA)  
We started by conducting Exploratory Data Analysis (EDA) to discover patterns and spot anomalies with the help of summary statistics. We noticed that some **predictor columns contained only zeroes and had negligible values of variance** [32 predictors in total]. After removing them from the dataset, we focused on **removing correlated variables** [correlation > 0.9]. 

#### ii. Defining New Variables 
Additionally, based on the first few models, we noticed that the boolean vectors denoting if a video had a low/low-mid/mid-high subscriber base, view base, average growth rate, and video count were significant predictors. Thus, for each of these different metrics, we created a single factor with multiple levels: 1 - denotes a 1 in the "low" column; 2 - denotes a 1 in the “low-mid” column; 3 - denotes a 1 in the “mid-high” column. Essentially, we created four new categorical predictors out of twelve boolean predictors - *Num_Subscribers, Num_Views, avg_growth, and count_vids*. This allowed us to **centralize and strengthen the predictive power of these variables**. 

#### iii. Feature Selection 
We then focused on narrowing down predictors. After using basic processing methods (transformation and combining predictors) and dimensionality reduction methods (based on correlation and variance) we were still left with $100+$ predictors. It was **difficult to obtain great results by with unsupervised methods** like PCA as they **failed to take into account the information that existed between feature values and the target variable** (growth_2_6). Since we found initial success with random forests we searched for feature selection methods specific to them. We ultimately decided on using the Boruta Algorithm - a wrapper method for random forest models that captures the most important predictors in a dataset.

### b. Statistical Model 

#### i. Tuning
As previously mentioned, after exploring various models (gradient boosting, linear regression, SVM), we found the most success with random forest models. Since the botura algorithm gave us the 47 best predictors we now needed to focus on tuning the model. 

To determine the number of variables available for splitting at each tree node **(mtry)**, we used out-of-bag cross validation and chose the number of trees using RMSE. Next, using a loop we tried to find the best number of trees **(ntree)** to grow and the minimum number of observations in a terminal node **(nodesize)**. 

#### ii. Model Description 

After tuning our final model is based on 47 predictors (determined using the Boruta algorithm) with mtry = 15, ntree = 500, and nodesize = 5. Essentially, this means that to predict growth_2_6 we used an ensembling machine learning algorithm that creates multiple decision trees - using the parameters above - and then combines the output generated by each of the decision trees.  

A decision tree is a classification or regression model which works on the concept of information gain at every node. For example, a single value can run through the entire tree based on some binary classification (T/F) until it reaches a terminal node; the final prediction is the average of the value of the dependent variable in that particular node. Figure 2 is an illustrative example of part of a decision tree that could be in our model. We see that at each node a decision is to be made and at the terminal nodes we have a prediction value. 

The second model we submitted combined the above random forest model with a gradient boosting model (ntrees = 2516, interaction depth = 5, $\lambda$ = 0.01). Gradient boosting is different from random forests in two ways: Random forests builds each tree independently while gradient boosting builds one tree at a time. This additive nature helps weak trees improve their shortcomings by introducing another weak tree. Secondly, random forests combine results at the end of processing by averaging while gradient boosting combines results along the way. **Since random forests achieve accuracy by reducing bias, and gradient boosting achieves accuracy by reducing variance, we hypothesized that averaging their predictions might help increase accuracy on both fronts**. Therefore, we predicted growth_2_6 with each model separately and then submitted the average of those predictions to Kaggle.
